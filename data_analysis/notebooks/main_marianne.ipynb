{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%reset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import sys; sys.path.insert(1, '../')\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import mne\r\n",
    "from actionpy.prepro import map_channels, raw_to_epochs, get_data, preprocessing\r\n",
    "%load_ext autoreload\r\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload module is not an IPython extension.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UsageError: Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "subject = 'S00'\r\n",
    "\r\n",
    "base_path ='C:\\\\Users\\\\Marianne\\\\Desktop\\\\SCP-ACT\\\\raw_data\\\\' + '\\\\' + subject + '\\\\' + subject + '_'\r\n",
    "files = ['rest_01', 'rest_02', 'exp_01', 'drmt_01']\r\n",
    "\r\n",
    "ending = '.vhdr'\r\n",
    "\r\n",
    "raws = []\r\n",
    "for file in files:\r\n",
    "    pth_raw = base_path + file + ending\r\n",
    "    print(pth_raw)\r\n",
    "    raws.append( preprocessing(pth_raw) ) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C:\\Users\\Marianne\\Desktop\\SCP-ACT\\raw_data\\\\S00\\S00_rest_01.vhdr\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\Marianne\\Desktop\\SCP-ACT\\data_analysis\\actionpy\\prepro\\prepro.py:14: RuntimeWarning: Online software filter detected. Using software filter settings and ignoring hardware values\n",
      "  raw = mne.io.read_raw_brainvision(pth, preload = True, verbose = 0)\n",
      "c:\\Users\\Marianne\\Desktop\\SCP-ACT\\data_analysis\\actionpy\\prepro\\prepro.py:14: RuntimeWarning: Channels contain different highpass filters. Lowest (weakest) filter setting (0.00 Hz) will be stored.\n",
      "  raw = mne.io.read_raw_brainvision(pth, preload = True, verbose = 0)\n",
      "c:\\Users\\Marianne\\Desktop\\SCP-ACT\\data_analysis\\actionpy\\prepro\\prepro.py:14: RuntimeWarning: Channels contain different lowpass filters. Highest (weakest) filter setting (500.00 Hz, Nyquist limit) will be stored.\n",
      "  raw = mne.io.read_raw_brainvision(pth, preload = True, verbose = 0)\n",
      "c:\\Users\\Marianne\\Desktop\\SCP-ACT\\data_analysis\\actionpy\\prepro\\prepro.py:17: RuntimeWarning: DigMontage is only a subset of info. There are 1 channel position not present in the DigMontage. The required channels are:\n",
      "\n",
      "['Resp'].\n",
      "\n",
      "Consider using inst.set_channel_types if these are not EEG channels, or use the on_missing parameter if the channel positions are allowed to be unknown in your analyses.\n",
      "  raw.set_montage('standard_1020',  on_missing = 'warn', verbose = 0)  # verbose for less output text\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:    5.7s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Not setting metadata\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=4)]: Done  32 out of  32 | elapsed:    9.3s finished\n",
      "c:\\Users\\Marianne\\Desktop\\SCP-ACT\\data_analysis\\actionpy\\prepro\\prepro.py:33: RuntimeWarning: The unit for channel(s) VEOG has changed from C to V.\n",
      "  raw.set_channel_types(mapping)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  7.0min finished\n",
      "C:\\Users\\Marianne\\anaconda3\\envs\\py_eeg\\lib\\site-packages\\autoreject\\ransac.py:226: RuntimeWarning: No bad channels to interpolate. Doing nothing...\n",
      "  epochs.interpolate_bads(reset_bads=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from actionpy.prepro import get_data\r\n",
    "%matplotlib qt\r\n",
    "markers = [[2, 4], [2, 4], [2, 6], [None, 4]]\r\n",
    "picks = ['Cz', 'Resp']\r\n",
    "\r\n",
    "datas = []\r\n",
    "for raw, marker in zip(raws, markers):\r\n",
    "    # raw.plot(duration=60, scalings=dict(eeg=120e-6))\r\n",
    "    datas.append( get_data(raw.copy().filter(None, 1, n_jobs=-1).resample(100), marker[0], marker[1], picks) )\r\n",
    "    # datas is now a list with the cut and filtered data from channels Cz and Resp"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=4)]: Done  31 out of  31 | elapsed:    4.1s finished\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Used Annotations descriptions: ['New Segment/', 'Stimulus/S  2', 'Stimulus/S  4']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "plt.figure()\r\n",
    "ylim = (-9e-5, 9e-5)\r\n",
    "refreq = 100\r\n",
    "n = len(datas)\r\n",
    "for i in range(n):\r\n",
    "    num = n*100 + 10 + i+1\r\n",
    "    plt.subplot(num)\r\n",
    "    plt.plot(raws[i].copy().resample(refreq).times[:len(datas[i][0, :])], np.std(datas[i][:, :], axis=0))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Global Descriptors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "### skip ###\r\n",
    "\r\n",
    "from actionpy.analysis import run_length_encoding, omega_descriptor, sigma_descriptor, phi_descriptor, average_phase_synchrony\r\n",
    "\r\n",
    "metrics = (omega_descriptor, sigma_descriptor, phi_descriptor, average_phase_synchrony)\r\n",
    "metrics_names = ('omega', 'sigma', 'phi', 'phase_synchrony')\r\n",
    " results = {channel: dict() for channel in channels}\r\n",
    "results = dict()\r\n",
    "# for i, channel in enumerate(channels):\r\n",
    "\r\n",
    "for metric, name in zip(metrics, metrics_names):\r\n",
    "    results[name] = [[np.real(metric(portion, sfreq=100)) if name=='phi' else np.real(metric(portion)) \\\r\n",
    "                        for portion in np.array_split(data, n, axis=1)] \r\n",
    "                        for data in datas]\r\n",
    "    \r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "cannot import name 'sampleentropy' from 'actionpy.analysis' (c:\\Users\\Marianne\\Desktop\\SCP-ACT\\data_analysis\\actionpy\\analysis\\__init__.py)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0a96c055fefa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mactionpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalysis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrun_length_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0momega_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_phase_synchrony\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleentropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0momega_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_phase_synchrony\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleentropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmetrics_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'omega'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sigma'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'phi'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'phase_synchrony'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rms'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sample entropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'sampleentropy' from 'actionpy.analysis' (c:\\Users\\Marianne\\Desktop\\SCP-ACT\\data_analysis\\actionpy\\analysis\\__init__.py)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "### skip ###\r\n",
    "\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "plt.figure(figsize=(4, 9))\r\n",
    "ylim = (-9e-5, 9e-5)\r\n",
    "refreq = 100\r\n",
    "n = len(metrics)\r\n",
    "for i in range(n):\r\n",
    "    num = n*100 + 10 + i+1\r\n",
    "    plt.subplot(num)\r\n",
    "    sns.boxplot(data=results[metrics_names[i]])\r\n",
    "    plt.xticks(ticks=np.arange(len(files)), labels=files)\r\n",
    "    plt.title(metrics_names[i])\r\n",
    "\r\n",
    "\r\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "### skip ###\r\n",
    "\r\n",
    "from actionpy.analysis import run_length_encoding\r\n",
    "\r\n",
    "run_length_encoding(datas[0][0], gridsize=1, plotme=True)\r\n",
    "np.array_split(datas[0][0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "243.35260115606937"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from nolds import sampen\r\n",
    "\r\n",
    "n = 14     # wir teilen jeden Datensatz in 14 Teile, sodass wir Teile von 30s bekommen. Damit können wir Statistik für jede Person machen\r\n",
    "\r\n",
    "results = dict()\r\n",
    "metrics = (run_length_encoding, sampen, np.std)   # np.std for rms\r\n",
    "metric_names = ('run length', 'complexity', 'rms')\r\n",
    "\r\n",
    "\r\n",
    "for metric, metric_name in zip(metrics, metric_names):\r\n",
    "    results[metric_name] = []\r\n",
    "    for condition in range(len(datas)):\r\n",
    "        pot = []      # we will put all results (for every metric, condition and 30s part) into list \"pot\"\r\n",
    "        \r\n",
    "        for i in range(n):     # that way, we actually split the data for every condition and metric into 30s parts\r\n",
    "            if metric_name == 'run length':\r\n",
    "                a = metric(np.array_split(datas[condition][0], n)[i], gridsize=1)\r\n",
    "            else:\r\n",
    "                a = metric(np.array_split(datas[condition][0], n)[i])\r\n",
    "            pot.append(a)\r\n",
    "\r\n",
    "        results[metric_name].append(pot)    # here, the results saved in the pot are put into the results dict, at the keys metric_name (which we loop through) so we have a dict with data for every metric\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import seaborn as sns\r\n",
    "#metric_name = 'run length'\r\n",
    "plt.figure()\r\n",
    "#sns.violinplot(data=results[metric_name])\r\n",
    "#plt.title(metric_name)\r\n",
    "\r\n",
    "plt.subplot(3,1,1)\r\n",
    "\r\n",
    "sns.violinplot(data=results['run length'])\r\n",
    "plt.title('run length')\r\n",
    "\r\n",
    "plt.subplot(3,1,2)\r\n",
    "sns.violinplot(data=results['complexity'])\r\n",
    "plt.title('complexity')\r\n",
    "\r\n",
    "plt.subplot(3,1,3)\r\n",
    "sns.violinplot(data=results['rms'])\r\n",
    "plt.title('complexity')\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'complexity')"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('eegenv': virtualenvwrapper)"
  },
  "interpreter": {
   "hash": "b3ab6a524f8fdefa2fba1a0ab7407e8dbd386bc97b991bfb50d01a11f6cd0d5f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}